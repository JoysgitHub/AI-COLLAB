{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoysgitHub/AI-COLLAB/blob/main/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "nOSkVjn5go-a",
        "outputId": "3e2aea3f-b986-44df-a46a-2dd1a7ea1125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/85\n",
            "1500/1500 [==============================] - 327s 215ms/step - loss: 0.1464 - accuracy: 0.9594 - val_loss: 0.0903 - val_accuracy: 0.9772\n",
            "Epoch 2/85\n",
            "1500/1500 [==============================] - 314s 209ms/step - loss: 0.0711 - accuracy: 0.9810 - val_loss: 0.0642 - val_accuracy: 0.9856\n",
            "Epoch 3/85\n",
            "1500/1500 [==============================] - 317s 211ms/step - loss: 0.0535 - accuracy: 0.9854 - val_loss: 0.0892 - val_accuracy: 0.9793\n",
            "Epoch 4/85\n",
            "1500/1500 [==============================] - 315s 210ms/step - loss: 0.0513 - accuracy: 0.9865 - val_loss: 0.0762 - val_accuracy: 0.9846\n",
            "Epoch 5/85\n",
            " 903/1500 [=================>............] - ETA: 2:05 - loss: 0.0398 - accuracy: 0.9891"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-34711f50446b>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m history = model.fit(train_images, train_labels, epochs=85,\n\u001b[0m\u001b[1;32m     82\u001b[0m                     validation_data=(val_images, val_labels))\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Trains the model for 2 epochs using the training data and also evaluates it on the test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m       \u001b[0mwithout_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnew_tracing_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0mexecution_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"notTraced\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwithout_tracing\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"traced\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mexperimental_get_tracing_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_primary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_primary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf  # Imports TensorFlow, a machine learning library\n",
        "from tensorflow.keras import datasets, layers, models  # Imports specific submodules from Keras for datasets, layers, and models\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix  # Imports performance metrics from scikit-learn\n",
        "import numpy as np  # Imports NumPy, a library for numerical operations\n",
        "import matplotlib.pyplot as plt  # Imports Pyplot from Matplotlib for plotting graphs\n",
        "import time  # Imports the time module to measure execution time\n",
        "import seaborn as sns  # Imports Seaborn, a library for statistical data visualization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "# This line loads the MNIST dataset, which is a collection of 28x28 pixel grayscale images of handwritten digits, along with their corresponding labels.\n",
        "\n",
        "# Add a channel dimension, and normalize pixel values to be between 0 and 1\n",
        "train_images = np.expand_dims(train_images, axis=-1) / 255.0\n",
        "test_images = np.expand_dims(test_images, axis=-1) / 255.0\n",
        "# These lines add an extra dimension to the image arrays to represent the color channel (even though MNIST is grayscale) and normalize the pixel values to be between 0 and 1 for better neural network performance.\n",
        "\n",
        "\n",
        "\n",
        "# Split the training data into training and validation sets (80-20 split)\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "# This line records the current time, which will be used later to calculate the total execution time of the model training.\n",
        "\n",
        "# Data augmentation\n",
        "# strategy to increase the diversity of your training data and reduce overfitting\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "])\n",
        "# This block defines a data augmentation strategy using a sequential model that includes random horizontal flips,\n",
        "# rotations, and zooms to the images. Data augmentation helps prevent overfitting by artificially increasing the diversity of the training data.\n",
        "\n",
        "\n",
        "# Define the convolutional base\n",
        "model = models.Sequential() #Initializes a new sequential model. Sequential models are a linear stack of layers.\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Adds a 2D convolutional layer with 64 filters, a 3x3 kernel size, 'relu' activation function,\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "# Add fully connected layers\n",
        "model.add(layers.Flatten()) # # Changes the shape of the data to a single long list without changing its contents.\n",
        "model.add(layers.Dense(512, activation='relu')) # Adds a layer with 512 neurons that can capture complex patterns.\n",
        "model.add(layers.Dropout(0.5)) # Randomly ignores half of the neurons in the previous layer to improve robustness.\n",
        "model.add(layers.Dense(10, activation='softmax'))  # Adds a layer that decides which of the 10 digit classes the image belongs to.\n",
        "\n",
        "\n",
        "\n",
        "# Learning rate scheduling\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,  # Start with this learning rate\n",
        "    decay_steps=10000,          # Adjust the rate every 10,000 steps\n",
        "    decay_rate=0.9              # Reduce the rate to 90% of the current rate\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "# Use the Adam optimizer with the above learning rate schedule\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',  # Use this loss function for multi-class classification\n",
        "              metrics=['accuracy'])                    # Track accuracy during training\n",
        "\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "# Stops training when the model doesn't improve for 5 epochs and restores the best weights.\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=100,\n",
        "                    validation_data=(val_images, val_labels))\n",
        "# Trains the model for 2 epochs using the training data and also evaluates it on the test data.\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()  # Record the end time of model evaluation\n",
        "execution_time = end_time - start_time  # Calculate the total execution time\n",
        "# Calculates how long it took to train and evaluate the model.\n",
        "\n",
        "\n",
        "# Evaluate on validation data\n",
        "val_loss, val_acc = model.evaluate(val_images, val_labels)\n",
        "print(f\"Validation accuracy: {val_acc * 100:.2f} %\")\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "y_pred1 = model.predict(test_images)\n",
        "y_pred = np.argmax(y_pred1, axis=1)\n",
        "precision = precision_score(test_labels, y_pred, average='macro')\n",
        "recall = recall_score(test_labels, y_pred, average='macro')\n",
        "f1 = f1_score(test_labels, y_pred, average='macro')\n",
        "print(f\"Precision: {precision * 100:.2f} %\")\n",
        "print(f\"Recall: {recall * 100:.2f} %\")\n",
        "print(f\"F1-score: {f1 * 100:.2f} %\")\n",
        "\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")  # Print total execution time\n",
        "\n",
        "\n",
        "\n",
        "# Create and visualize the confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=[0,1,2,3,4,5,6,7,8,9], yticklabels=[0,1,2,3,4,5,6,7,8,9])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Plot accuracy over time\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='b')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='r')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}